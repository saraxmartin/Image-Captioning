ENCODER:  EncoderCNN(
  (base_model): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
    (classifier): Identity()
  )
  (embed): Linear(in_features=25088, out_features=1024, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
  (prelu): PReLU(num_parameters=1)
)
DECODER:  DecoderLSTM(
  (embedding): Embedding(77, 1024)
  (lstm): LSTM(2048, 1024)
  (attention): AoA_GatedAttention(
    (attn): Linear(in_features=1024, out_features=126, bias=True)
    (context): Linear(in_features=126, out_features=1, bias=True)
    (attn_on_attn): Linear(in_features=1, out_features=126, bias=True)
    (context_on_attn): Linear(in_features=126, out_features=1, bias=True)
    (gate): Sigmoid()
  )
  (fc_out): Linear(in_features=1024, out_features=77, bias=True)
)



ENCODER:
0.Images shape:  torch.Size([16, 3, 256, 256])
1.Features shape:  torch.Size([16, 2048, 8, 8])
2.Features shape:  torch.Size([16, 8, 8, 2048])
3.Features shape:  torch.Size([16, 64, 2048])
4.Features shape:  torch.Size([16, 64, 512])

DECODER:
0.Embedded captions: torch.Size([16, 11, 512])
1.Context vector: torch.Size([16, 512])
1.Attention weigths: torch.Size([16, 64, 1])
2. Context after expanding to seq len size: torch.Size([16, 11, 512])
3. LSTM input: torch.Size([16, 11, 1024])
3. LSTM output: torch.Size([16, 11, 512])